<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>m5 Forecasting Accuracy Challenge</title>
    <meta charset="utf-8" />
    <meta name="author" content="Kevin Ferris" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# m5 Forecasting Accuracy Challenge
## Estimate the unit sales of Walmart retail goods
### Kevin Ferris
### Boston Red Sox
### September, 2021

---

class: inverse, center, middle

# Background

---

# m5 Forecasting - Accuracy



- [Kaggle competition](https://www.kaggle.com/c/m5-forecasting-accuracy/) that ran in 2020
- Participants were given daily sales totals for 3,000 items across 10 stores from 2011-2015
- Asked to forecast sales for the next 15 days

![m5 Forecasting Data Structure](data-flow-chart.png "m5 Forecasting Data Structure")

Image credit: [Dipanshu Rana](https://dipanshurana.medium.com/m5-forecasting-accuracy-1b5a10218fcf)

---

# Examples

Test

---

# Overview

I stumbled across [this notebook](https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda) from Kaggle user `headsortails` which introduced me to the competition.  `headsortails` highlights, amongst many other things, that there are six obvious predictors available each of which will have different effects for each store/item.

- Time trend: how sales have changed over time
- Month: sales vary by month
- Day of Week
- SNAP effects: certain days in each state are eligible for SNAP benefits
- Event days: days with special events such as Christmas or the Super Bowl
- Item price
- Natural disasters

---

# Results

*"Pretty clear that if we cannot beat 0.75 significantly we are on the wrong track."* - [Matthias](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/164599)


|Entry             | Score|Rank    |
|:-----------------|-----:|:-------|
|YeonJun IN_STU    | 0.521|#1      |
|Matthias          | 0.528|#2      |
|mf                | 0.536|#3      |
|Baseline (ES_bu)  | 0.671|Top 8%  |
|Baseline (ARIMAX) | 0.691|Top 10% |
|Basline (sNaive)  | 0.847|Top 40% |

---
class: inverse, center, middle

# My Approach

---

# Model Type

This seemed like an interesting challenge that was a good fit for my skillset.  Unlikely most Kaggle competitions, I didn't think that complicated machine learning algorithms were a good fit here for three reasons:

1. It's a hierarchical dataset across stores and items, with time series data.  Approaches would need to account for that structure.
2. While the full dataset has millions of observations, I don't think this is a "big dataset" because we only have ~2000 days worth of data for each individual item at a specific store.
3. The only obvious non-linearity appears to be the overall time trend, and there aren't any obvious interactions.

Instead, I thought the most important part of this problem would be capturing the effects that `headsortails` proposed specific to each item at each store.  This seemed like a classic fit or a hierarchical model.

---

# Model Specification

Ideally, I would fit a two-part model that would appropriately regularize each effect for each specific item/store.  I started with a baseline of just a time trend, month, and weekday effects then progressively added more to the model.

```
m_non0 &lt;- glmer(
  I(sales != 0) ~ f(day) + factor(month) + factor(weekday) + ... + 
    (1 + f(day) + factor(month) + factor(weekday) + ... | store_id) + 
    (1 + f(day) + factor(month) + factor(weekday) + ... | store_id) + 
    (1 + f(day) + factor(month) + factor(weekday) + ... | store_id:item_id), 
  data = data
  family = binomial()
)
m_sales &lt;- glmer(
  sales ~ f(day) + factor(month) + factor(weekday) + ... + 
    (1 + f(day) + factor(month) + factor(weekday) + ... | store_id) + 
    (1 + f(day) + factor(month) + factor(weekday) + ... | store_id) + 
    (1 + f(day) + factor(month) + factor(weekday) + ... | store_id:item_id), 
  data = filter(data, sales &gt; 0), 
  family = binomial()
)
```

---

# Limitations

I wanted to challenge myself to 

- (a) do a Kaggle competition in R on just my laptop[^1]
- (b) use nothing more complicated than Generalized Linear Models
- (c) avoid reviewing the Kaggle discussion and code until completion

[^1]: 16 GB RAM, i7-9750H CPU

---

# My Approach

1. Fit a "global" model to the entire dataset
2. Estimate effects for each specific store, then each specific item, then each item at a specific store  
3. Regularize these noisy effects by applying Regression to the Mean

I started with just a time trend, monthly, and day-of-week effects for a baseline model then iteratively made it more complicated.

---

background-image: url(bda3-cover.png)

???

---

# Data Wrangling



---

# Global Model

```
# Model Specification
f_base &lt;- formula(~ s(day, k = 10, bs = "ts") + 
                    factor(month) + weekday + snap_CA + snap_TX + snap_WI)

# Pr(sales &gt; 0)
m_non0_base &lt;- gam(update(f_base, cbind(non0, n - non0) ~ .), 
                   data = train_dat, 
                   family = binomial())

# E(sales | sales &gt; 0)
m_sales_base &lt;- gam(update(f_base, sales ~ .), 
                    data = filter(train_dat, sales &gt; 0), 
                    family = poisson(), 
                    offset = log(non0))
```

---

# Time Trend

This ended up being more complicated than I expected.  I initially used a polynomial trend, but switched to GAMs because they can algorithmically apply shrinkage.  This broke the rules because it's more complicated than a GLM, but it's a minor infraction.  

```
data_i &lt;- data %&gt;% filter(specified store/item)
m_non0_time_i &lt;- gam(I(sales != 0) ~ s(day, k = 8, bs = "ts"), 
                     data = data_i, 
                     family = binomial(), 
                     offset = lp_non0_base)
m_sales_time_i &lt;- gam(sales ~ s(day, k = 8, bs = "ts"), 
                      data = data_i %&gt;% filter(sales &gt; 0), 
                      family = poisson(), 
                      offset = lp_sales_base)
```

---

# Monthly Effects

Note that a "better" approach may have been to use a seasonal trend, but this appeared to work reasonably well.

```
m_non0_month_i &lt;- gam(I(sales != 0) ~ factor(month), 
                      data = data_i, 
                      family = binomial(), 
                      offset = lp_non0_base)
m_sales_month_i &lt;- gam(sales ~ factor(month), 
                       data = data_i %&gt;% filter(sales &gt; 0), 
                       family = poisson(), 
                       offset = lp_sales_base)
```

---

# Day of Week Effects

Straightforward

```
m_non0_weekday_i &lt;- gam(I(sales != 0) ~ factor(weekday), 
                        data = data_i, 
                        family = binomial(), 
                        offset = lp_non0_base)
m_sales_weekday_i &lt;- gam(sales ~ factor(weekday), 
                         data = data_i %&gt;% filter(sales &gt; 0), 
                         family = poisson(), 
                         offset = lp_sales_base)
```

---
class: inverse, center, middle

# My Results

---

# Baseline results

I was pretty happy with this initial model.  It only used 3 of the 7 predictors, but still outperformed all the baselines and was in the top 10% of submissions.



|Entry             | Score|Rank    |
|:-----------------|-----:|:-------|
|YeonJun IN_STU    | 0.521|#1      |
|Matthias          | 0.528|#2      |
|Alan Lahoud       | 0.536|#5      |
|KF Baseline       | 0.669|#400    |
|Baseline (ES_bu)  | 0.671|Top 8%  |
|Baseline (ARIMAX) | 0.691|Top 10% |
|Basline (sNaive)  | 0.847|Top 40% |

---

# Adding Complexity

From there, I added in complexity.  My attempts are in order of most -&gt; least complex, so the first (best) model listed includes all the changes of the below models.


|Entry            | Score|Rank |Notes              |
|:----------------|-----:|:----|:------------------|
|YeonJun IN_STU   | 0.521|#1   |                   |
|Alan Lahoud      | 0.536|#5   |                   |
|KF GAMs          | 0.605|#111 |Use GAM time trend |
|KF SNAP          | 0.607|#121 |Add SNAP effects   |
|KF Baseline      | 0.669|#400 |                   |
|Baseline (ES_bu) | 0.671|#415 |                   |


---
class: inverse, center, middle

# Validation

---

# My Validation Approach

1. Accuracy
2. Calibration
3. Smell Test
4. Feedback

---

# Accuracy

---

# Calibration

---

# Smell Test

---

# Feedback


---
class: inverse, center, middle

# Concluding Thoughts

---

# What I Learned

---
background-image: url(https://github.com/yihui/xaringan/releases/download/v0.0.2/karl-moustache.jpg)
---

# Additional Work

---

# Conclusion




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
